# Initialization Techniques in Deep Neural Networks 

This assignment explores how different initialization methods affect the training of deep neural networks.  
It is part of **Week 1 (Course 2: Improving Deep Neural Networks – Hyperparameter Tuning, Regularization, and Optimization)** from the **Deep Learning Specialization** by **Andrew Ng** on Coursera.

##  Description

In this lab, I implemented three different initialization techniques — **Zero Initialization**, **Random Initialization**, and **He Initialization** — and compared their impact on the training speed and final accuracy of a deep neural network.  
Good initialization is critical for avoiding vanishing or exploding gradients and achieving faster convergence.

### Key Concepts Covered:
- Importance of initialization in neural networks
- Zero initialization and its failure
- Random initialization and issues of instability
- He initialization for ReLU-based deep networks
- Analyzing training curves and convergence behavior

##  Files Included

- `initialization_lab.ipynb`: Main notebook containing the implementation and experiments
- `initialization_utils.py`: Helper functions to create datasets, train models, and plot decision boundaries
- `data/`: Synthetic datasets used for classification

> ⚠️ This repository includes only my original solutions and follows Coursera’s Honor Code.

##  Tools Used

- Python 3
- NumPy
- Matplotlib
- Jupyter Notebook

##  Course Info

This lab is part of:
> [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)  
> Instructor: **Andrew Ng**  
> Course 2: Improving Deep Neural Networks  
> Week 1: Initialization

##  License

This repository is created for educational and portfolio purposes only. Please do not use it for direct Coursera submissions.

---

 If you're passionate about deep learning optimization, consider starring this repository!
